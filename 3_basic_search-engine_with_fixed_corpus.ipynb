{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from pandas.tests.frame.test_sort_values_level_as_str import ascending\n",
    "\n",
    "import os,re,math,string,copy,json\n",
    "import requests,joblib\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from docutils.nodes import document\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'Severity', 'Description', 'Recommendation',\n",
      "       'Associated Findings:'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def Documents():\n",
    "    \n",
    "    #read the document first \n",
    "    path = '/Users/srijeetchatterjee/Desktop/Projects/ATT/Search_Engine/'\n",
    "    documents = pd.read_excel(path + 'documents/Veracode-Report.xlsx')\n",
    "    \n",
    "    print(documents.columns)\n",
    "    # drop the na rows as they do not hold any value at all \n",
    "    documents = documents.dropna()\n",
    "    documents['Description'] = documents['Description'].str.strip()\n",
    "    documents['Description'] = documents['Description'].str.replace('\\\\n', ' ')\n",
    "    documents['Description'] = documents['Description'].str.replace('\\\\t', ' ')\n",
    "    \n",
    "    documents['Description'] = documents['Description'].apply(\n",
    "        lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    lemmatizer_wordnet = WordNetLemmatizer()\n",
    "    \n",
    "    documents['Description'] = documents['Description'].apply(lambda x: \" \".join(\n",
    "        [lemmatizer_wordnet.lemmatize(word, pos = 'v') for word in x.split()]))\n",
    "    \n",
    "\n",
    "    tokens = [nltk.word_tokenize(token) for token in documents['Description']]\n",
    "    \n",
    "    \n",
    "    Docdictionary = corpora.Dictionary(tokens)\n",
    "#     print(type(Docdictionary))\n",
    "#     print(Docdictionary)\n",
    "    \n",
    "    Doccorpus = [Docdictionary.doc2bow(token) for token in tokens]\n",
    "    \n",
    "#     print(type(Doccorpus))\n",
    "#     print(Doccorpus)\n",
    "    \n",
    "    Doctf_idf = gensim.models.TfidfModel(Doccorpus)\n",
    "#     print(type(Doctf_idf))\n",
    "#     print(Doctf_idf)\n",
    "    \n",
    "    Docsims = gensim.similarities.Similarity(None, Doctf_idf[Doccorpus], num_features = len(Docdictionary))\n",
    "#     print(type(Docsims))\n",
    "#     print(Docsims)\n",
    "    \n",
    "    joblib.dump(Docdictionary, path+'documents/pickle/Docdictonary.pkl')\n",
    "    joblib.dump(Doctf_idf, path+'documents/pickle/Doctf_idf.pkl')\n",
    "    joblib.dump(Docsims, path+'documents/pickle/Docsims.pkl')\n",
    "\n",
    "    documents.to_csv(path +'documents/pickle/vulnerable_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'Severity', 'Description', 'Recommendation',\n",
      "       'Associated Findings:'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Datacleaning(documents):\n",
    "    documents['Description'] = documents['Description'].str.replace('\\n', ' ')\n",
    "    documents['Description'] = documents['Description'].str.replace('\\xa0', ' ')\n",
    "    documents['Description'] = documents['Description'].str.replace('\\r', ' ')\n",
    "\n",
    "    documents['Description'] = documents['Description'].apply(\n",
    "        ''.join).str.replace(r'[^A-Za-z\\s\\n]+', '')\n",
    "\n",
    "    documents['Description'] = token_lemitization(documents['Description'])\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lemitization(Data):\n",
    "    \n",
    "    text_tokenised = []\n",
    "    punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "    \n",
    "    for text in Data:\n",
    "        text = punctuation.sub('', text)\n",
    "        text_tokenised.append(word_tokenize(text.lower()))\n",
    "\n",
    "    text_lemitized = []\n",
    "\n",
    "    lemmatizer_wordnet = WordNetLemmatizer()\n",
    "\n",
    "    for text_list in text_tokenised:\n",
    "        word_lemitized = []\n",
    "        for text in text_list:\n",
    "            if text != '':\n",
    "                word_lemitized.append(\n",
    "                    lemmatizer_wordnet.lemmatize(text, pos='v'))\n",
    "        text_lemitized.append(word_lemitized)\n",
    "\n",
    "    text_string = []\n",
    "\n",
    "    for text in text_lemitized:\n",
    "        text_string.append(' '.join(text))\n",
    "    \n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryProcessing(query):\n",
    "    query = query.lower()\n",
    "    query = query.replace(r'[^A-Za-z\\s\\n]+', '')\n",
    "    lemmatizer_wordnet = WordNetLemmatizer()\n",
    "    query = \" \".join(lemmatizer_wordnet.lemmatize(word, pos='v')for word in query.split())\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/srijeetchatterjee/Desktop/Projects/ATT/Search_Engine/documents/pickle/'\n",
    "Docdict = joblib.load(path + 'Docdictonary.pkl')\n",
    "Doctfidf = joblib.load(path + 'Doctf_idf.pkl')\n",
    "Doctsimilarity = joblib.load(path + 'Docsims.pkl')\n",
    "Docdoc = pd.read_csv(path + 'vulnerable_baseline.csv', na_filter=False)\n",
    "\n",
    "\n",
    "def vulDocSearch(query):\n",
    "    \n",
    "    query = queryProcessing(query)\n",
    "    \n",
    "    token_prob = nltk.word_tokenize(query)\n",
    "    \n",
    "    # tokenized user query is matched with the pickle file(which is bow) and creatinga doc2bow dictionary.\n",
    "    doc2bow = Docdict.doc2bow(token_prob)\n",
    "#     print(f\"doc2bow : {doc2bow}\")\n",
    "#     print('\\n')\n",
    "\n",
    "    # dictionary created is converted into tfidf.\n",
    "    doc_tfidf = Doctfidf[doc2bow]\n",
    "#     print(f\"doc_tfidf : {doc_tfidf}\")\n",
    "#     print('\\n')\n",
    "    \n",
    "    # taking similarity of user query from the Doctsimilarity.pkl\n",
    "    similarity = Doctsimilarity[doc_tfidf]\n",
    "#     print(f\"similarity : {similarity}\")\n",
    "#     print('\\n')\n",
    "    \n",
    "    # sorting similarity in descending order\n",
    "    indices = similarity.argsort()[::-1]\n",
    "    \n",
    "    dde = Docdoc.iloc[indices]\n",
    "    \n",
    "    score = similarity[indices]\n",
    "    dde['Prediction%'] = np.round(score*100)  # to get the % in the /100 format\n",
    "    \n",
    "    dde = dde.loc[dde['Prediction%'] > 0]\n",
    "    dde = dde[0:75]\n",
    "\n",
    "    # selecting the fields which we want to show on the screen.\n",
    "    dde = dde[['Prediction%', 'Name','Description', 'Recommendation', 'Severity']]\n",
    "    \n",
    "    return dde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL injection vulnerabilities occur This allows an attacker to\n",
      "The searched results are : \n",
      "   Prediction%                           Name  \\\n",
      "0         38.0                  SQL Injection   \n",
      "3          7.0                 CRLF Injection   \n",
      "2          5.0           Cross-Site Scripting   \n",
      "1          3.0            Directory Traversal   \n",
      "5          3.0  Insufficient Input Validation   \n",
      "\n",
      "                                         Description  \\\n",
      "0  sql injection vulnerabilities occur when data ...   \n",
      "3  the acronym crlf stand for \"carriage return, l...   \n",
      "2  cross-site script (xss) attack occur when an a...   \n",
      "1  allow user input to control paths use in files...   \n",
      "5  weaknesses in this category be relate to an ab...   \n",
      "\n",
      "                                      Recommendation Severity  \n",
      "0  Several techniques can be used to prevent SQL ...     High  \n",
      "3  Apply robust input filtering for all user-supp...   Medium  \n",
      "2  Several techniques can be used to prevent XSS ...   Medium  \n",
      "1  Assume all user-supplied input is malicious. V...   Medium  \n",
      "5  Validate input from untrusted sources before i...   Medium  \n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "print('The searched results are : ')\n",
    "print(vulDocSearch(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
